{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/aleksejfilippov/Desktop/Python_projects/data_for_topic model/voted-kaggle-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = get_stop_words('en')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'can', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'universit√©', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
     ]
    }
   ],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in df['Description'].iteritems():\n",
    "    # clean and tokenize document string\n",
    "    raw = str(i[1]).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "#     stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
    "    \n",
    "    # remove word containing only single char\n",
    "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(new_lemma_tokens)\n",
    "\n",
    "# sample data\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.052861832, 'data'),\n",
      "   (0.014257352, 'context'),\n",
      "   (0.014034216, 'acknowledgement'),\n",
      "   (0.013728653, 'content'),\n",
      "   (0.012447229, 'inspiration')],\n",
      "  -0.3231673813476864),\n",
      " ([(0.028079802, 'data'),\n",
      "   (0.017174728, 'dataset'),\n",
      "   (0.012816708, 'can'),\n",
      "   (0.01055987, 'file'),\n",
      "   (0.008779279, 'content')],\n",
      "  -0.703904492106824),\n",
      " ([(0.026912026, 'data'),\n",
      "   (0.013638603, 'year'),\n",
      "   (0.011611627, 'state'),\n",
      "   (0.011277071, 'country'),\n",
      "   (0.007728388, 'dataset')],\n",
      "  -1.0970631177976942),\n",
      " ([(0.027837401, 'image'),\n",
      "   (0.026654799, 'model'),\n",
      "   (0.017786022, 'trained'),\n",
      "   (0.0146734435, 'dataset'),\n",
      "   (0.01201019, 'feature')],\n",
      "  -1.128134631237387),\n",
      " ([(0.034772683, 'player'),\n",
      "   (0.02310619, 'team'),\n",
      "   (0.020060651, 'game'),\n",
      "   (0.012765648, 'match'),\n",
      "   (0.011703315, 'season')],\n",
      "  -1.1990156781152272),\n",
      " ([(0.009213146, 'de'),\n",
      "   (0.008106104, 'pokemon'),\n",
      "   (0.007277282, 'game'),\n",
      "   (0.006948655, 'time'),\n",
      "   (0.005284475, 'dataset')],\n",
      "  -1.2158541195012968),\n",
      " ([(0.025667034, 'city'),\n",
      "   (0.01636032, 'york'),\n",
      "   (0.015010862, 'new'),\n",
      "   (0.011255413, 'trip'),\n",
      "   (0.010931806, 'nyc')],\n",
      "  -1.5433884908609965),\n",
      " ([(0.014300576, 'word'),\n",
      "   (0.012540488, 'language'),\n",
      "   (0.0112086, 'corpus'),\n",
      "   (0.008310443, 'speech'),\n",
      "   (0.0065396, 'message')],\n",
      "  -1.9532477629828482),\n",
      " ([(0.015983952, 'http'),\n",
      "   (0.012293961, 'context'),\n",
      "   (0.011921423, 'name'),\n",
      "   (0.011521858, 'license'),\n",
      "   (0.010735125, 'health')],\n",
      "  -1.9593669626955221),\n",
      " ([(0.011618333, 'dataset'),\n",
      "   (0.01016613, 'data'),\n",
      "   (0.005496386, 'information'),\n",
      "   (0.004698913, 'class'),\n",
      "   (0.00460975, 'attribute')],\n",
      "  -2.0405297115809953),\n",
      " ([(0.083711304, 'university'),\n",
      "   (0.018634504, 'state'),\n",
      "   (0.012742291, 'csv'),\n",
      "   (0.0116445515, 'college'),\n",
      "   (0.011081033, 'word')],\n",
      "  -2.5175474499788315),\n",
      " ([(0.110529535, 'dataset'),\n",
      "   (0.087730706, 'description'),\n",
      "   (0.08074313, 'yet'),\n",
      "   (0.013196575, 'question'),\n",
      "   (0.0107125705, 'score')],\n",
      "  -2.65147647205027),\n",
      " ([(0.009027056, 'reading'),\n",
      "   (0.0076820464, 'real'),\n",
      "   (0.0073027, 'ultrasound'),\n",
      "   (0.0058298684, 'angle'),\n",
      "   (0.0057701697, 'child')],\n",
      "  -2.990406571334553),\n",
      " ([(0.09600352, 'type'),\n",
      "   (0.09542663, 'name'),\n",
      "   (0.09283707, 'com'),\n",
      "   (0.08882985, 'bea'),\n",
      "   (0.08868059, 'mxbean')],\n",
      "  -4.0173259520490365),\n",
      " ([(0.016177913, 'customer'),\n",
      "   (0.010635796, 'station'),\n",
      "   (0.009688852, 'kumar'),\n",
      "   (0.0066802213, 'child'),\n",
      "   (0.006574319, 'name')],\n",
      "  -10.359531343200183)]\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=20)\n",
    "import pprint\n",
    "pprint.pprint(ldamodel.top_topics(corpus,topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
